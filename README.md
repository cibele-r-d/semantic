### HyperionDev Data Science Task intro to Spacy and NLP
---

# Semantic 

### Comparing Words:  
#### Output:  
- **small model (en_core_web_sm):**
  - cat monkey 0.677
  - banana monkey 0.727
  - banana cat 0.680

- **medium model (en_core_web_md):**
  - cat monkey 0.592
  - banana monkey 0.404
  - banana cat 0.223

**Observation:**   
In the comparison, it's observed that the utilization of small-scale models contributes minimally to the differentiation between nouns. While the medium-scale models offer better differentiation, similarity, and analysis overall, they still struggle to provide significant differentiation between nouns, as evidenced by the lower similarity scores for comparisons such as "banana monkey" and "banana cat". The higher similarity scores between "banana monkey" compared to "banana cat" and "cat monkey" in both models could indeed be attributed to the common association between monkeys and bananas as a preferred food source. This association is often ingrained in common knowledge and cultural understanding, potentially influencing the similarity scores generated by the models.


### Comparing Sentences:
**Sentence to compare:**  
"Why my cat is on the car"

**Output:**  
- where did my dog go - 0.630
- Hello, there is my car - 0.803
- I've lost my car in my car - 0.678
- I'd like my boat back - 0.562
- I will name my dog Diana - 0.649

Each sentence in the array is compared with the sentence "Why my cat is on the car" using the medium model. Among the comparisons, the highest similarity score is achieved with the sentence "Hello, there is my car". This suggests that, based on the context and semantics captured by the model, "Hello, there is my car" is the most similar sentence to "Why my cat is on the car" among the given options.

**Possible Contributing Factors**

**1. Common Keywords:** Both sentences contain the word "car", which may contribute significantly to their similarity score. The presence of this common keyword likely influences the model's perception of their semantic similarity. 

**2. Contextual Similarity:** The context provided by the word "car" in both sentences may align closely with the context of the query sentence "Why my cat is on the car". This contextual similarity could lead the model to assign a higher similarity score to "Hello, there is my car".

**3. Semantic Understanding:** The model may recognize patterns or relationships in the language that suggest a closer semantic connection between "Why my cat is on the car" and "Hello, there is my car" compared to the other sentences. This could be related to how the words are used together and the overall meaning conveyed by the sentences.

**4. Syntax and Structure:** The syntactic structure and arrangement of words in "Hello, there is my car" might be more similar to "Why my cat is on the car" compared to the other sentences, influencing the model's assessment of their similarity.

On another note, the lower similarity score between "I've lost my car in my car" and "Why my cat is on the car" compared to "Hello, there is my car" can be attributed to the lack of semantic coherence, contextual relevance, and syntactic differences between the sentences. These factors contribute to the model's assessment of similarity and influence the observed differences in similarity scores.


### Sugesting a movie based on sinopse similarity:

**Movie watched:** Planet Hulk
**Recommendation:** Suspiria

The outputs demonstrate the effectiveness of the recommendation system, where the movie with the highest similarity score is suggested as the next watch. However, it's also evident that the recommendation might not always align perfectly with human intuition, as observed in the example where "Suspiria" is recommended after watching "Planet Hulk". Fine-tuning the comparison mechanism or incorporating additional features could enhance the precision of recommendations.



